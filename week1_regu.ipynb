{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n_feat, n_beta, n_obs, rho, noise, test_size):\n",
    "    \n",
    "    # Correlated normal random variables (ad-hoc)\n",
    "    corr_mat = np.ones([n_feat, n_feat]) * rho\n",
    "    np.fill_diagonal(corr_mat, 1)\n",
    "    C = np.linalg.cholesky(corr_mat)\n",
    "    X = np.matmul(np.random.normal(size=[n_obs, n_feat]), np.transpose(C))\n",
    "    \n",
    "    # Linearly dependent observations\n",
    "    beta = np.random.binomial(1, np.min([1.0, n_beta/n_feat]), n_feat) \n",
    "    beta = beta / np.sum(beta)\n",
    "    eps = noise * np.random.normal(size=[n_obs])\n",
    "    y = np.matmul(X, beta) + eps\n",
    "    \n",
    "    # Train and test data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    return  x_train, x_test, y_train, y_test, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 2\n",
    "n_beta = 1\n",
    "n_obs = 60*7\n",
    "rho = 0.0\n",
    "noise = 1.0\n",
    "test_size = 0.2\n",
    "\n",
    "np.random.seed(123)\n",
    "x_train, x_test, y_train, y_test, beta_true = gen_data(n_feat, n_beta, n_obs, rho, noise, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Target returns\") \n",
    "plt.xlabel(\"time t\") \n",
    "plt.ylabel(\"r_t\") \n",
    "plt.plot(range(y_train.size), y_train, label='train') \n",
    "plt.plot(range(y_train.size, y_train.size + y_test.size), y_test, label='test') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Feature variables\") \n",
    "plt.xlabel(\"time t\") \n",
    "plt.ylabel(\"x_t\") \n",
    "plt.plot(np.concatenate((x_train, x_test)), label='train') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_closedform(X, y):\n",
    "    Xtr = np.transpose(X)\n",
    "    XtrX = np.matmul(Xtr, X)\n",
    "    if np.linalg.det(XtrX) == 0.0:\n",
    "        sys.exit('Matrix is not invertible')\n",
    "    beta = np.matmul(np.linalg.inv(XtrX),  np.matmul(Xtr, y)) \n",
    "    return beta\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 100\n",
    "n_beta = 10\n",
    "n_obs = 60*7\n",
    "rho = 0.0\n",
    "noise = 1.0\n",
    "test_size = 0.2\n",
    "\n",
    "np.random.seed(123)\n",
    "x_train, x_test, y_train, y_test, beta_true = gen_data(n_feat, n_beta, n_obs, rho, noise, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = beta_closedform(x_train, y_train)\n",
    "loss_train = rmse(y_train, np.matmul(x_train, beta_hat)) \n",
    "loss_test = rmse(y_test, np.matmul(x_test, beta_hat)) \n",
    "\n",
    "plt.title(\"OLS beta parameters\") \n",
    "plt.xlabel(\"train\") \n",
    "plt.ylabel(\"true\") \n",
    "plt.plot(beta_hat, beta_true,\"ob\") \n",
    "plt.show()\n",
    "\n",
    "print(\"Train RMSE: \\t %.4f\\nTest RMSE: \\t %.4f\" % (loss_train, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_true, \"ob\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average RMSE for different feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 100\n",
    "n_beta = 10\n",
    "n_obs = 60*7\n",
    "rho = 0.15\n",
    "noise = 1.0\n",
    "test_size = 0.2\n",
    "n_seed = 10\n",
    "\n",
    "res_train = np.zeros(shape=[n_seed, n_feat-1])\n",
    "res_test = np.zeros(shape=[n_seed, n_feat-1])\n",
    "\n",
    "for s in range(n_seed):\n",
    "    \n",
    "    np.random.seed(s)\n",
    "    x_train, x_test, y_train, y_test, beta_true = gen_data(n_feat, n_beta, n_obs, rho, noise, test_size)\n",
    "    \n",
    "    for n in range(n_feat-1):\n",
    "        nc = n + 1\n",
    "        beta_hat = beta_closedform(x_train[:,:nc], y_train)\n",
    "        loss_train = rmse(y_train, np.matmul(x_train[:,:nc], beta_hat)) \n",
    "        loss_test = rmse(y_test, np.matmul(x_test[:,:nc], beta_hat)) \n",
    "        res_train[s, n] = loss_train\n",
    "        res_test[s, n] = loss_test\n",
    "        \n",
    "y_train = np.nanmean(res_train, axis=0)\n",
    "y_test = np.nanmean(res_test, axis=0)\n",
    "x = range(1, n_feat)\n",
    "\n",
    "plt.title(\"Average RMSE on train and test samples\") \n",
    "plt.xlabel(\"number of features\") \n",
    "plt.ylabel(\"RMSE\") \n",
    "plt.plot(x, y_train, label='train') \n",
    "plt.plot(x, y_test, label='test') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, beta):\n",
    "    return tf.matmul(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_error(y_pred, y_target):\n",
    "    return tf.reduce_mean(tf.square(y_target - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(W, reg_type):\n",
    "    if reg_type == \"OLS\":\n",
    "        loss = 0.0\n",
    "    elif reg_type == 'LASSO':\n",
    "        loss = tf.reduce_sum(tf.abs(W))\n",
    "    elif reg_type == 'Ridge':\n",
    "        loss = tf.reduce_sum(tf.square(W))\n",
    "    else:\n",
    "        print('Invalid regression_type parameter value', file=sys.stderr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_model(X, y, X_test, y_test, learning_rate, epochs, reg_type=\"OLS\", lbd=0.0, log=True):\n",
    "\n",
    "    # Change tensors format\n",
    "    X, y, X_test, y_test = [data.astype('float32') for data in [X, y, X_test, y_test]]\n",
    "    y = y.reshape([-1,1])\n",
    "    y_test = y_test.reshape([-1,1])\n",
    "    \n",
    "    # Number of features\n",
    "    num_features = np.size(X,1)\n",
    "    \n",
    "    # Define variable\n",
    "    beta = tf.Variable(tf.ones([num_features, 1])/num_features, name=\"beta\") \n",
    "    \n",
    "    # Set optimizer\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)\n",
    "    \n",
    "    ## Loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Wrap computation inside a GradientTape for automatic differentiation\n",
    "        with tf.GradientTape() as g:\n",
    "            y_pred = linear_regression(X, beta)\n",
    "            loss_train = prediction_error(y_pred, y) + lbd * regularization(beta, reg_type)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = g.gradient(loss_train, [beta])\n",
    "\n",
    "        # Update beta following gradients\n",
    "        optimizer.apply_gradients(zip(gradients, [beta]))\n",
    "        \n",
    "        # Log\n",
    "        if (log & (epoch % 10 == 0)):\n",
    "            y_pred = linear_regression(X_test, beta)\n",
    "            loss_test = prediction_error(y_pred, y_test) + pen_param * regularization(beta, reg_type)\n",
    "            print(\"At epoch %d: \\t Train loss: %.4f \\t Test loss: %.4f\" % (epoch, float(loss_train), float(loss_test)))\n",
    "    \n",
    "    # Compute last loss values\n",
    "    y_pred = linear_regression(X, beta)\n",
    "    loss_train = prediction_error(y_pred, y) + lbd * regularization(beta, reg_type)\n",
    "    y_pred = linear_regression(X_test, beta)\n",
    "    loss_test = prediction_error(y_pred, y_test) + pen_param * regularization(beta, reg_type)\n",
    "    \n",
    "    return beta.numpy(), loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 300\n",
    "n_beta = 50\n",
    "n_obs = 60*7\n",
    "rho = 0.0\n",
    "noise = 1.0\n",
    "test_size = 0.25\n",
    "\n",
    "#np.random.seed(123)\n",
    "x_train, x_test, y_train, y_test, beta_true = gen_data(n_feat, n_beta, n_obs, rho, noise, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate, epochs, reg_type, pen_param = [0.01, 100, \"OLS\", 0.0]\n",
    "\n",
    "# Run\n",
    "beta_hat, loss_train, loss_test = make_train_model(x_train, y_train, x_test, y_test, learning_rate, epochs, reg_type, pen_param)\n",
    "\n",
    "plt.title(\"LS beta parameters\") \n",
    "plt.xlabel(\"train\") \n",
    "plt.ylabel(\"true\") \n",
    "plt.plot(beta_hat, beta_true,\"ob\") \n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Histogram of parameter values\") \n",
    "plt.hist(beta_hat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 20, 20)\n",
    "res_train = []\n",
    "res_test = []\n",
    "\n",
    "for lbd in lambdas:\n",
    "    # Parameters\n",
    "    learning_rate, epochs, reg_type, pen_param = [0.01, 100, \"Ridge\", lbd]\n",
    "    # Run\n",
    "    beta_hat, loss_train, loss_test = make_train_model(x_train, y_train, x_test, y_test, \n",
    "                                                       learning_rate, epochs, reg_type, pen_param,\n",
    "                                                       log=False)\n",
    "    # Save\n",
    "    res_train.append(loss_train)\n",
    "    res_test.append(loss_test)\n",
    "    \n",
    "plt.title(\"RMSE for different penalty parameter\") \n",
    "plt.xlabel(\"lambda\") \n",
    "plt.ylabel(\"RMSE\") \n",
    "plt.plot(lambdas, res_train, label='train') \n",
    "plt.plot(lambdas, res_test, label='test') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
