{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation & strategy backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "We follow the same steps as last week, but with different cross-validation approaches.\n",
    "\n",
    "1. Import the data\n",
    "2. Feature engineering and data labelling\n",
    "3. Split the data into train, validation, and test datasets\n",
    "4. Model builder\n",
    "5. Train and cross-validate the model\n",
    "6. Make predictions and evaluate the performance of the final selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data\n",
    "\n",
    "We will import data from Yahoo! finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cryptocompare\n",
    "import pandas as pd\n",
    "\n",
    "df = cryptocompare.get_historical_price_hour('BTC', curr='USD', limit=2000)\n",
    "df = pd.DataFrame(df)\n",
    "df.time = pd.to_datetime(df['time'], unit='s')\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "(ggplot(df, aes(x='df.index', y='close'))\n",
    " + geom_line()\n",
    " + xlab('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering and data labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "\n",
    "df['ADX'] = ta.ADX(df['high'].values, df['low'].values, df['close'].values, timeperiod=14) / 30\n",
    "df['RSI'] = ta.RSI(df['close'].values, timeperiod=14) / 30\n",
    "df['SMA'] = ta.SMA(df['close'].values, timeperiod=20) / 1e4\n",
    "df['SMA2'] = ta.SMA(df['volumeto'].values, timeperiod=20) / 1e7\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data labelling\n",
    "\n",
    "We will use the **fixed horizon method** with a non-zero threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_window = 5\n",
    "return_threshold = 0.0025\n",
    "\n",
    "## Compute the n-day future returns\n",
    "df['fut_returns'] = df['close'].pct_change(+label_window).shift(-label_window)\n",
    "\n",
    "## Attribute the class {-1, 0, 1} if the future return is {below, between, above} the thresholds\n",
    "df['target_class'] = np.where(df.fut_returns > return_threshold, 1, \n",
    "                                np.where(df.fut_returns < -return_threshold, -1, 0))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that our target classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(ggplot(df, aes(x='target_class'))\n",
    " + geom_histogram())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Extract $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "## feature variables\n",
    "predictors_list = ['ADX', 'RSI', 'SMA', 'SMA2']\n",
    "X = df[predictors_list].to_numpy()\n",
    "\n",
    "## target variable\n",
    "y = df.target_class.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split the data into train-test datasets\n",
    "\n",
    "This time we **DO NOT** shuffle the data and reserve the last part as a **holdout** sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the two samples balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of 1s in the train and test sets: %.2f and %.2f' % (np.mean(y_train==1)*100, np.mean(y_test==1)*100))\n",
    "print('Percentage of -1s in the train and test sets: %.2f and %.2f' % (np.mean(y_train==-1)*100, np.mean(y_test==-1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model builder\n",
    "\n",
    "Functions to easily build and evaluate models. *This is normally a big chunk of code!!!*\n",
    "\n",
    "For simplicity, we use scikit-learn but the steps would be the same with TensorFlow but the model construction would be a bit more involved.\n",
    "\n",
    "We will work with a Random Forest, see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def create_model(n_estimators, max_depth):\n",
    "    return RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and cross-validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## model parameters\n",
    "n_estimators = 100\n",
    "max_depth = 10\n",
    "## CV parameters\n",
    "n_split = 5\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X_train):\n",
    "    \n",
    "    ## CV train-test split\n",
    "    x_cv_train, x_cv_test = X_train[train_index], X_train[test_index]\n",
    "    y_cv_train, y_cv_test = y_train[train_index], y_train[test_index]\n",
    "  \n",
    "    ## create and train the model\n",
    "    model = create_model(n_estimators, max_depth)\n",
    "    model.fit(x_cv_train, y_cv_train)\n",
    "    \n",
    "    ## collect accuracy metrics\n",
    "    scores_train.append(model.score(x_cv_train, y_cv_train, sample_weight=None))\n",
    "    scores_test.append(model.score(x_cv_test, y_cv_test, sample_weight=None))\n",
    "\n",
    "print('Mean accuracy on CV train %.2f%%' % (100*np.mean(scores_train)))\n",
    "print('Mean accuracy on CV test %.2f%%' % (100*np.mean(scores_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "  * Are the results good? If no, why?\n",
    "  * How to perform hyper-parameter tuning?\n",
    "  * Is the cross-validation approach appropriate here? If no, what would you change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that generate some random parameters\n",
    "def generate_random_hyperparams(par1_min, par1_max, par2_min, par2_max):\n",
    "    random_par1 = np.random.uniform(par1_min, par1_max)\n",
    "    random_par2 = np.random.uniform(par2_min, par2_max)\n",
    "    return random_par1, random_par2\n",
    "\n",
    "## create grid of parameters with 'meshgrid' as follows\n",
    "np.array(np.meshgrid([1, 2, 3], [4, 5], [6, 7])).T.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make predictions and evaluate the performance of the final selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "n_estimators = 10\n",
    "model = create_model(n_estimators, max_depth)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Mean accuracy on train %.2f%%' % (100*model.score(X_train, y_train, sample_weight=None)))\n",
    "print('Mean accuracy on holdout %.2f%%' % (100*model.score(X_test, y_test, sample_weight=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting a betting rule\n",
    "\n",
    "Assume that we have an infinite amount of cash available.\n",
    "\n",
    "We will bet at most 1'000 USD every hour on the next 5-hour prediction and hold it until the final time. Hence, at a given time, we will bet at most 5'000 dollars.\n",
    "\n",
    "We will invest this dollar proportionally to the confidence, or informativeness, of our signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "## Predicted class along with with probability\n",
    "signal = model.predict(X_test)\n",
    "proba = np.max(np.exp(model.predict_log_proba(X_test)), axis=1)\n",
    "\n",
    "## n-day returns\n",
    "fut_returns = df.tail(y_test.size).fut_returns\n",
    "\n",
    "## Put together\n",
    "df_strat = pd.DataFrame(data={'signal':signal, 'proba':proba, 'fut_returns':fut_returns.to_numpy()},\n",
    "                       index=fut_returns.index)\n",
    "\n",
    "df_strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the predicted class\n",
    "(ggplot(df_strat, aes(x='df_strat.index', y='signal')) + \n",
    "   geom_point() + \n",
    "   xlab('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Compute the bet size and side (long/short) and the realized PnL\n",
    "bet_max = 1000\n",
    "df_strat['z'] = (df_strat.proba - 1/3) / np.sqrt(df_strat.proba * (1 - df_strat.proba))\n",
    "df_strat['position'] = bet_max * df_strat.signal * (2*norm.cdf(df_strat.z) - 1)\n",
    "df_strat['pnl'] = df_strat.fut_returns * df_strat.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our total position looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "\n",
    "df_strat['total_position'] = ta.SMA(df_strat['position'], 5)\n",
    "\n",
    "(ggplot(df_strat, aes(x='df_strat.index', y='total_position')) + \n",
    "  geom_line() + xlab(\"time\") + ggtitle(\"Total long/short position\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we make any money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strat['cum_pnl'] = np.cumsum(df_strat.pnl)\n",
    "\n",
    "(ggplot(df_strat, aes(x='df_strat.index', y='cum_pnl')) + \n",
    "  geom_line() + xlab(\"time\") + ggtitle(\"Strategy cumulative PnL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df.tail(y_test.size), aes(x='df.tail(y_test.size).index', y='close')) + geom_line())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "  * What do you think of the performance of this strategy? Would you go to production with it? Why?\n",
    "  * How would you make this backtest more realistic? (e.g. transaction costs)\n",
    "  * What alternative betting strategy would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "In a future lecture we will study how to identify which features are important, or not. This is particularly useful during the model development stage as it will help you better understand and build your model.\n",
    "\n",
    "Herebelow is just a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "    \n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "print(predictors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
